# ü§ñ Top 5 AI Papers This Week
## Week of August 08, 2025

Welcome to this week's roundup of the most impactful AI research papers! These papers have been generating buzz across Reddit, academic Twitter, and research communities.

**üìä This Week's Stats:**
- üìÑ **5 featured papers** from **1 categories**  
- üë• **41 contributing authors**
- üî• **Average engagement score:** 25.0
- üèÜ **Highest scorer:** 25 points

---

## 1. Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning
  for Large Language Models

üß† **Category:** CS.AI | üìÖ **Published:** August 07, 2025 | üî• **Score:** 25 points

**Authors:** Haitao Hong, Yuchen Yan, Xingyu Wu et al. (+5 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2508.05613v1) | [PDF Download](https://arxiv.org/pdf/2508.05613v1.pdf)

Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities.. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards..

However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking.. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model.. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model.. This design enhances robustness and mitigates the risk of reward hacking.. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model.. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input.. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size.. We conduct reinforcement learning using both VerifyRM and Cooper..

Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct.. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL..

---

## 2. Shuffle-R1: Efficient RL framework for Multimodal Large Language Models
  via Data-centric Dynamic Shuffle

üß† **Category:** CS.AI | üìÖ **Published:** August 07, 2025 | üî• **Score:** 25 points

**Authors:** Linghao Zhu, Yiran Guan, Dingkang Liang et al. (+6 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2508.05612v1) | [PDF Download](https://arxiv.org/pdf/2508.05612v1.pdf)

Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM).. However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time..

These issues lead to suboptimal gradient updates and hinder long-term learning efficiency.. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition.. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling..

Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead.. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM..

---

## 3. The World According to LLMs: How Geographic Origin Influences LLMs'
  Entity Deduction Capabilities

üß† **Category:** CS.AI | üìÖ **Published:** August 07, 2025 | üî• **Score:** 25 points

**Authors:** Harsh Nishant Lalai, Raj Sanjay Shah, Jiaxin Pei et al. (+3 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2508.05525v1) | [PDF Download](https://arxiv.org/pdf/2508.05525v1.pdf)

Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data.. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves..

The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose.. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions.. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish).. Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East.. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities.. Notably, the language in which the game is played has minimal impact on performance gaps.. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups..

By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes.. We release the dataset (Geo20Q+) and code at https://sites.google.com/view/llmbias20q/home..

---

## 4. Auto-Eval Judge: Towards a General Agentic Framework for Task Completion
  Evaluation

üß† **Category:** CS.AI | üìÖ **Published:** August 07, 2025 | üî• **Score:** 25 points

**Authors:** Roshita Bhonsle, Rishav Dutta, Sneha Vavilapalli et al. (+8 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2508.05508v1) | [PDF Download](https://arxiv.org/pdf/2508.05508v1.pdf)

The increasing adoption of foundation models as agents across diverse domains necessitates a robust evaluation framework.. Current methods, such as LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step reasoning that drives agentic decision-making..

Meanwhile, existing Agent-as-a-Judge systems, where one agent evaluates another's task completion, are typically designed for narrow, domain-specific settings.. To address this gap, we propose a generalizable, modular framework for evaluating agent task completion independent of the task domain.. The framework emulates human-like evaluation by decomposing tasks into sub-tasks and validating each step using available information, such as the agent's output and reasoning.. Each module contributes to a specific aspect of the evaluation process, and their outputs are aggregated to produce a final verdict on task completion.. We validate our framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA and BigCodeBench..

Our Judge Agent predicts task success with closer agreement to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy, respectively, compared to the GPT-4o based LLM-as-a-Judge baseline.. This demonstrates the potential of our proposed general-purpose evaluation framework..

---

## 5. InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs
  to Enhance Reasoning Capabilities

üß† **Category:** CS.AI | üìÖ **Published:** August 07, 2025 | üî• **Score:** 25 points

**Authors:** Shuo Cai, Su Lu, Qi Zhou et al. (+4 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2508.05496v1) | [PDF Download](https://arxiv.org/pdf/2508.05496v1.pdf)

Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks.. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost..

Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability.. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning.. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics.. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources.. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks.. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks.. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks..

Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner.. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT..

---


## üìà About This Analysis

Each week, I analyze recent AI papers from ArXiv and rank them based on:

üó£Ô∏è **Social Media Engagement** - Mentions and discussions on Reddit  
üéØ **Research Impact Indicators** - Trending keywords and methodologies  
üë• **Collaboration Signals** - Author networks and institutional diversity  
‚è∞ **Recency Factor** - Boost for just-published papers  

**Methodology:** Papers are scored using a composite algorithm that weighs social media mentions (Reddit discussions, estimated Twitter activity) alongside content analysis for breakthrough keywords like "transformer," "multimodal," "reasoning," and others that typically indicate high-impact research.

**Coverage:** This analysis scans 7 major AI categories on ArXiv: Artificial Intelligence, Machine Learning, Natural Language Processing, Computer Vision, Neural Networks, Robotics, and Statistics ML.

---

*ü§ñ This analysis is automatically generated every Friday by monitoring ArXiv submissions and tracking social media engagement.*

**üì¨ Subscribe** for weekly AI research updates  
**üí¨ Share your thoughts** on this week's selections in the comments  
**üîó Follow the project** on [GitHub](https://github.com/kjanik70/ai-papers-agent)

*Next edition: August 15, 2025*
