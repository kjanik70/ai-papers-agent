# ü§ñ Top 5 AI Papers This Week
## Week of December 12, 2025

Welcome to this week's roundup of the most impactful AI research papers! These papers have been generating buzz across Reddit, academic Twitter, and research communities.

**üìä This Week's Stats:**
- üìÑ **5 featured papers** from **1 categories**  
- üë• **112 contributing authors**
- üî• **Average engagement score:** 25.0
- üèÜ **Highest scorer:** 25 points

---

## 1. Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation

üß† **Category:** CS.AI | üìÖ **Published:** December 11, 2025 | üî• **Score:** 25 points

**Authors:** Yiwen Tang, Zoey Guo, Kaixin Zhu et al. (+11 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2512.10949v1) | [PDF Download](https://arxiv.org/pdf/2512.10949v1.pdf)

Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently.. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures..

This makes 3D generation significantly sensitive to reward designs and RL algorithms.. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles.. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement..

We hope this study provides insights into RL-driven reasoning for 3D generation.. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1..

---

## 2. BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models

üß† **Category:** CS.AI | üìÖ **Published:** December 11, 2025 | üî• **Score:** 25 points

**Authors:** Shengao Wang, Wenqi Wang, Zecheng Wang et al. (+20 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2512.10932v1) | [PDF Download](https://arxiv.org/pdf/2512.10932v1.pdf)

Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models.. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation..

The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences.. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities..

Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks.. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models..

---

## 3. SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale

üß† **Category:** CS.AI | üìÖ **Published:** December 11, 2025 | üî• **Score:** 25 points

**Authors:** Max Zimmer, Christophe Roux, Moritz Wagner et al. (+2 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2512.10922v1) | [PDF Download](https://arxiv.org/pdf/2512.10922v1.pdf)

The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters.. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures..

State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data.. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics.. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale.. To that end, we decouple the rows by enforcing equal sparsity levels per row.. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data..

Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free.. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures..

---

## 4. LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification

üß† **Category:** CS.AI | üìÖ **Published:** December 11, 2025 | üî• **Score:** 25 points

**Authors:** Michael Schlee, Christoph Weisser, Timo Kivim√§ki et al. (+2 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2512.10793v1) | [PDF Download](https://arxiv.org/pdf/2512.10793v1.pdf)

LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks.. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users..

Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction.. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost..

---

## 5. The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality

üß† **Category:** CS.AI | üìÖ **Published:** December 11, 2025 | üî• **Score:** 25 points

**Authors:** Aileen Cheng, Alon Jacovi, Amir Globerson et al. (+62 more)

**Links:** [ArXiv Paper](https://arxiv.org/abs/2512.10791v1) | [PDF Download](https://arxiv.org/pdf/2512.10791v1.pdf)

We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios.. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models..

Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality.. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity.. It can be found at https://www.kaggle.com/benchmarks/google/facts ..

---


## üìà About This Analysis

Each week, I analyze recent AI papers from ArXiv and rank them based on:

üó£Ô∏è **Social Media Engagement** - Mentions and discussions on Reddit  
üéØ **Research Impact Indicators** - Trending keywords and methodologies  
üë• **Collaboration Signals** - Author networks and institutional diversity  
‚è∞ **Recency Factor** - Boost for just-published papers  

**Methodology:** Papers are scored using a composite algorithm that weighs social media mentions (Reddit discussions, estimated Twitter activity) alongside content analysis for breakthrough keywords like "transformer," "multimodal," "reasoning," and others that typically indicate high-impact research.

**Coverage:** This analysis scans 7 major AI categories on ArXiv: Artificial Intelligence, Machine Learning, Natural Language Processing, Computer Vision, Neural Networks, Robotics, and Statistics ML.

---

*ü§ñ This analysis is automatically generated every Friday by monitoring ArXiv submissions and tracking social media engagement.*

**üì¨ Subscribe** for weekly AI research updates  
**üí¨ Share your thoughts** on this week's selections in the comments  
**üîó Follow the project** on [GitHub](https://github.com/kjanik70/ai-papers-agent)

*Next edition: December 19, 2025*
